{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CrE2V8a0XYre",
        "lvQnpcrfbO8W",
        "g9L1uNZ6Xc5B",
        "KM_5xCEWXftJ",
        "Zh4weFOFLlXV",
        "CannhZPjxnvd",
        "3xh2d3fpxxqO",
        "QavlS9ZN_2pf",
        "b-5Ec4PpVp-h",
        "Dsk9MiHxPJCK",
        "j9ULu-JKPXLw",
        "TCHT2laUPO0Y",
        "MPJPN2tBPvqi",
        "G_kK4wHwPZ9C",
        "Gg4m6u1gPhuR",
        "Jm3npGZDPzDy",
        "zWG1whu_RG_y",
        "9l3Sfw6rRPND",
        "g9s6O_daSVVj",
        "qOiERq6OSDyt",
        "kpPQmI_oSII1",
        "YhRZ51GqSMke",
        "CWnG_aBqTS1I",
        "EFH-gniaeVeo"
      ],
      "mount_file_id": "1mIjE1oJURifgldWgVbkE-Emwc6AV_wF2",
      "authorship_tag": "ABX9TyNfmu+9oETN/YU0feoKprPO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idanh8/IoT_project_accelerometer_data/blob/main/step_counter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "EFH-gniaeVeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "ipS7-1gffAia"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilities"
      ],
      "metadata": {
        "id": "wQyrywAceZby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(x, y, z):\n",
        "  return (x**2 + y**2 + z**2)**0.5"
      ],
      "metadata": {
        "id": "-ROCbQTxNBP0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_raw(zip_path):\n",
        "  count = 0\n",
        "  datasets = {}\n",
        "  with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    for file_name in z.namelist():\n",
        "      if file_name == '8_walk_4_3.csv':\n",
        "        continue\n",
        "      file_data = {}\n",
        "      count += 1\n",
        "      csv_data = z.read(file_name)\n",
        "\n",
        "      header = csv_data.decode().split('\\n')[:5]\n",
        "      header = [item.lower() for item in header]\n",
        "      header = [item.replace('\"', '') for item in header]\n",
        "      rows = csv_data.decode().split('\\n')[5:]\n",
        "      df = pd.read_csv(io.StringIO('\\n'.join(rows)))\n",
        "      activity = 0 if ('walk' in header[0] or 'walking' in header[2]) else 1\n",
        "      steps = int(header[3].split(',')[1])\n",
        "\n",
        "\n",
        "      file_data.update([('Name', file_name),('Data', df), ('Steps', steps), ('Activity', activity)])\n",
        "      exec(f\"datasets[{count-1}] = file_data\")\n",
        "  print(f'There are {count} files in the dataset')\n",
        "  problems = ['11_walk_5_1.csv', '6_run_3_1.csv', '6_run_4_1.csv','11_walk_1_1.csv', '11_walk_2_1.csv', '11_walk_3_1.csv', '6_walk_5_1.csv']\n",
        "  for dataset in datasets.values():\n",
        "    if dataset['Name'] in problems:\n",
        "      df = dataset['Data']\n",
        "      df = df.drop(0)\n",
        "      df = df.reset_index(drop=True)\n",
        "      dataset['Data'] = df\n",
        "    if dataset['Name'] == '16_run_3_1.csv':\n",
        "      df = dataset['Data']\n",
        "      df = df.drop(106)\n",
        "      df = df.reset_index(drop=True)\n",
        "      dataset['Data'] = df\n",
        "    if dataset['Name'] == '1_walk_4_1.csv':\n",
        "      df = dataset['Data']\n",
        "      df = df[:995]\n",
        "      df = df.reset_index(drop=True)\n",
        "      dataset['Data'] = df\n",
        "    if dataset['Name'] == '31_walk_2_1.csv':\n",
        "      df = dataset['Data']\n",
        "      df = df.drop(207)\n",
        "      df = df.reset_index(drop=True)\n",
        "      dataset['Data'] = df\n",
        "    if dataset['Name'] == '4_run_2_2.csv':\n",
        "      df = dataset['Data']\n",
        "      df = df.drop(185)\n",
        "      df = df.reset_index(drop=True)\n",
        "      dataset['Data'] = df\n",
        "    if dataset['Name'] == '4_walk_1_3.csv':\n",
        "      df = dataset['Data']\n",
        "      df = df.drop(368)\n",
        "      df = df.reset_index(drop=True)\n",
        "      dataset['Data'] = df\n",
        "    if dataset['Name'] == '4_walk_2_3.csv':\n",
        "      df = dataset['Data']\n",
        "      df = df.drop(95)\n",
        "      df = df.reset_index(drop=True)\n",
        "      dataset['Data'] = df\n",
        "    if dataset['Name'] == '4_walk_4_2.csv':\n",
        "      df = dataset['Data']\n",
        "      df = df.drop(599)\n",
        "      df = df.reset_index(drop=True)\n",
        "      dataset['Data'] = df\n",
        "    if dataset['Name'] == '4_walk_3_2.csv':\n",
        "      df = dataset['Data']\n",
        "      df = df.drop(39)\n",
        "      df = df.drop(41)\n",
        "      df = df.reset_index(drop=True)\n",
        "      dataset['Data'] = df\n",
        "    if dataset['Name'] == '5_run_3_1.csv':\n",
        "      df = dataset['Data']\n",
        "      df = df.drop(352)\n",
        "      df = df.reset_index(drop=True)\n",
        "      dataset['Data'] = df\n",
        "    if dataset['Name'] == '8_run_3_1.csv':\n",
        "      df = dataset['Data']\n",
        "      df = df.drop(264)\n",
        "      df = df.drop(605)\n",
        "      df = df.drop(606)\n",
        "      df = df.reset_index(drop=True)\n",
        "      dataset['Data'] = df\n",
        "    if dataset['Name'] == '8_run_3_1.csv':\n",
        "      df = dataset['Data']\n",
        "      df = df.drop(603)\n",
        "      df = df.drop(604)\n",
        "      df = df.drop(605)\n",
        "      df = df.reset_index(drop=True)\n",
        "      dataset['Data'] = df\n",
        "    df = dataset['Data']\n",
        "    df['Norm'] = df.apply(lambda row: norm(float(row[1]), float(row[2]), float(row[3])), axis =1)\n",
        "  return datasets"
      ],
      "metadata": {
        "id": "avivuWWifP2l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(data):\n",
        "    features = {}\n",
        "\n",
        "    # Statistical features\n",
        "    features['mean'] = data.mean()\n",
        "    features['std'] = data.std()\n",
        "    features['min'] = data.min()\n",
        "    features['max'] = data.max()\n",
        "    features['range'] = data.max() - data.min()\n",
        "\n",
        "    # Frequency domain features (using Fourier transform)\n",
        "    fft_data = np.fft.fft(data)\n",
        "    power_spectrum = np.abs(fft_data) ** 2\n",
        "    features['power_spectrum_mean'] = power_spectrum.mean()\n",
        "    features['power_spectrum_std'] = power_spectrum.std()\n",
        "\n",
        "    features['custom_feature'] = data.sum() * data.std()\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "c7lyVlN3fbGy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_and_extract(datasets):\n",
        "    extracted_features_list = []\n",
        "    y = []\n",
        "    for dataset in datasets.values():\n",
        "        aggregated_features = {}\n",
        "        df = dataset['Data'].astype(float)\n",
        "\n",
        "        for axis in [1, 2, 3, 4]:\n",
        "            axis_data = df.iloc[:,axis]\n",
        "            axis_features = extract_features(axis_data)\n",
        "\n",
        "            axis_features = {f'{axis}_{feature}': value for feature, value in axis_features.items()}\n",
        "\n",
        "            aggregated_features.update(axis_features)\n",
        "\n",
        "        aggregated_features['Act'] = dataset['Activity']\n",
        "        y.append(dataset['Steps'])\n",
        "        extracted_features_list.append(aggregated_features)\n",
        "\n",
        "    combined_df = pd.DataFrame(extracted_features_list)\n",
        "    nan_indices = combined_df.index[combined_df.isnull().any(axis=1)].tolist()\n",
        "    combined_df = combined_df.dropna()\n",
        "    del y[nan_indices[0]]\n",
        "    return combined_df, y"
      ],
      "metadata": {
        "id": "5b-caufcffL3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def single_combine_and_extract(dataset):\n",
        "    extracted_features_list = []\n",
        "    y = []\n",
        "    aggregated_features = {}\n",
        "    df = dataset['Data'].astype(float)\n",
        "\n",
        "    for axis in [1, 2, 3, 4]:\n",
        "        axis_data = df.iloc[:,axis]\n",
        "        axis_features = extract_features(axis_data)\n",
        "\n",
        "        axis_features = {f'{axis}_{feature}': value for feature, value in axis_features.items()}\n",
        "\n",
        "        aggregated_features.update(axis_features)\n",
        "\n",
        "    aggregated_features['Act'] = dataset['Activity']\n",
        "    y.append(dataset['Steps'])\n",
        "    extracted_features_list.append(aggregated_features)\n",
        "\n",
        "    combined_df = pd.DataFrame(extracted_features_list)\n",
        "    combined_df = combined_df.dropna()\n",
        "    return combined_df, y"
      ],
      "metadata": {
        "id": "iBnOHg3yiiTD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_predict(train_data, train_labels, test_data, test_labels):\n",
        "  scaler = StandardScaler()\n",
        "  data_scaled = scaler.fit_transform(train_data)\n",
        "\n",
        "  model = xgb.XGBRegressor(learning_rate=0.16743239807751675, max_depth=9, n_estimators=781)\n",
        "  model.fit(train_data, train_labels)\n",
        "\n",
        "  y_pred = model.predict(test_data)\n",
        "  for i in range(len(test_labels)):\n",
        "    print(f'True: {test_labels[i]}, Predicted: {y_pred[i]}')\n",
        "\n",
        "  mse = mean_squared_error(test_labels, y_pred)\n",
        "  mae = mean_absolute_error(test_labels, y_pred)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Mean Squared Error:\", mse)\n",
        "  print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "id": "UuJfSCUMNi5A"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and Predict"
      ],
      "metadata": {
        "id": "l_5CZhjpOTAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = 'data_set.zip' # change accordingly, please ensure a zipfile is passed containing csv files in the correct format\n",
        "data = process_raw(zip_path)\n",
        "train_data, train_labels = combine_and_extract(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRgx8Ub3gaWz",
        "outputId": "6c73d8f6-4f65-4f0d-bd92-725ecdce6a72"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 243 files in the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data, test_labels = single_combine_and_extract(data[130]) # change accordingly, please ensure correct format"
      ],
      "metadata": {
        "id": "NGc-pD5thJWl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_predict(train_data, train_labels, test_data, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW4k3mOEz0yI",
        "outputId": "8443538a-b9d3-4149-dadc-b72624e3071d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True: 130, Predicted: 130.0008087158203\n",
            "\n",
            "Mean Squared Error: 6.540212780237198e-07\n",
            "Mean Absolute Error: 0.0008087158203125\n"
          ]
        }
      ]
    }
  ]
}